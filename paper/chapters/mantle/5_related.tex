%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}									%%%%%%%%%%
\label{related-work}									%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Decouple data/metadata
Mantle decouples policy from mechanism in the metadata service to stabilize decision making. Much of the related work does not focus on the migration policies themselves and instead focuses on mechanisms for moving metadata. 

\textbf{Compute it - Hashing:} this distributes metadata evenly across MDS
nodes and clients find the MDS in charge of the metadata by applying a function
to a file identifier. PVFSv2~\cite{hildebrand:msst2005-pnfs} and
SkyFS~\cite{xing:sc2009-skyfs} hash the filename to locate the authority for
metadata. CalvinFS~\cite{thomson:fast2015-calvinfs} hashes the pathname to find
a database shard on a server. It handles many small files and fully
linearizable random writes using the feature rich Calvin database, which has
support for WAN/LAN replication, OLLP for mid-commit commits, and a
sophisticated logging subsystem. 

\textbf{Look it up - Table-based Mapping}: this is a form of hashing, where
indices are either managed by a centralized server or the clients. For example,
IBRIX~\cite{hp:whitepaper2012-storeall} distributes inode ranges round robin to
all servers and HBA~\cite{zhu:pds2008-hba} distributes metadata randomly to
each server and uses bloom filters to speedup the table lookups. These
techniques also ignore locality.



To further enhance scalability, many hashing schemes employ dynamic load balancing. ~\cite{li:msst2006-dynamic} presented dynamic balancing formulas to account for a forgetting factor, access information, and the number of MDS nodes in elastic clusters.~\cite{xing:sc2009-skyfs} used a master-slave architecture to detect low resource usage and migrated metadata using a consistent hashing-based load balancer. GPFS~\cite{schmuck:fast2002-gpfs} elects MDS nodes to manage metadata for different objects. Operations for different objects can operate in parallel and operations to the same object are synchronized. While this approach improves metadata parallelism, delegating management to different servers remains centralized at a token manager. This token manager can be overloaded with requests and large file system sizes - in fact, GPFS actively revokes tokens if the system gets too big. GIGA+~\cite{patil:fast2011-giga+} alleviates hotspots and ``flash crowds" by allowing unsynchronized directory growth for create intensive workloads. 

