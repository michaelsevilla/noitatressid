\chapter{Cudele: Subtree Consistency \& Durability Semantics}
\label{chp:cudele}

\begin{figure}[tb] \centering
\includegraphics[width=0.35\textwidth]{./chapters/cudele/figures/subtree-policies1.png}
\caption{Illustration of subtrees with different semantics co-existing in a
global namespace.  For performance, clients can relax consistency on their
subtree (HDFS) or decouple the subtree and move it locally (BatchFS, RAMDisk).
Decoupled subtrees can relax durability for even better performance.
}\label{fig:subtree-policies} \end{figure}

% What did we do
To address the scalability limits of namespace with global semantics, we
present an API and framework that lets developers dynamically control the
consistency and durability guarantees for subtrees in the file system
namespace.  Figure~\ref{fig:subtree-policies} shows a potential setup in our
proposed system where a single global namespace has subtrees for applications
optimized with techniques from different state-of-the-art architectures.  The
HDFS\footnote{HDFS itself is not directly evaluated in this paper, although the
semantics and their performance is explored in~\S\ref{sec:use-case-1}.} subtree
has weaker than strong consistency because it lets clients read files opened
for writing~\cite{hakimzadeh:dais14-hdfs-consistency}, which means that not all
updates are immediately seen by all clients; the BatchFS and RAMDisk subtrees
are decoupled from the global namespace and have similar consistency/durability
behavior to those systems; and the POSIX IO subtree retains the rigidity of
POSIX IO's strong consistency.  Subtrees without policies inherit the
consistency/durability semantics of the parent and future work will examine
embeddable or inheritable policies.

Our prototype system, Cudele, achieves this by exposing ``mechanisms" that
users combine to specify their preferred semantics.  Cudele supports 3 forms
of consistency (invisible, weak, and strong) and 3 degrees of durability (none,
local, and global) giving the user a wide range of policies and optimizations
that can be custom fit to an application. We make the following contributions:

\begin{enumerate}

  \item A framework and API for assigning consistency/durability policies 
  to subtrees in the file system namespace; this lets users navigate
  the trade-offs of different metadata protocols on the same storage system.

  \item This framework lets subtrees with different semantics co-exist in a
  global namespace. We show how this scales further and performs better than
  systems that use one strategy for the entire namespace .

  \item A prototype that lets users custom fit subtrees to applications
  dynamically. 

\end{enumerate}

The last contribution lays the groundwork for future work on our prototype. It
is more scalable than the current practice of mounting different storage systems in a global
namespace because there is no need to provision dedicated storage clusters to
applications or move data between these systems.  For example, the results of a
Hadoop job do not need to be migrated into a Ceph file system (CephFS) for other processing; instead
the user can change the semantics of the HDFS subtree into a CephFS subtree.
This may cause metadata/data movement to make things strongly consistent again
but this is superior to moving all data across file system boundaries. Our prototype
enables studies that adjust these semantics over {\it time and space}, where
subtrees can change their semantics and migrate around the cluster without ever
moving the data they reference.

% Results
The results in this paper pave the way for such a system and confirm the
assertions of ``clean-state" research that decouple namespaces; specifically
that these techniques drastically improve performance (we see 104\(\times\) speed up).
We go a step further by quantifying the costs of merging updates (7\(\times\)
slow down) and maintaining durability (\(10\times\) slow down). In our
prototype, we get an 8\(\times\) speedup and can scale to twice as many clients
when we assign a more relaxed form of consistency and durability to a subtree
with a create-heavy workload.  We use Ceph as a prototyping platform because it
is used in cloud-based and data center systems and has a presence in
HPC~\cite{wang:pdsw13-ceph-hpc}. 

%In the remainder of the paper, Section~\ref{sec:posix-overheads} quantifies the
%cost of POSIX IO consistency and system-defined durability and
%Section~\ref{sec:methodology-decoupled-namespaces} presents the Cudele
%prototype and API. Section~\ref{sec:implementation} describes the Cudele
%mechanisms and shows how re-using internal subsystems results in an
%implementation of less than 500 lines of code. The evaluation in
%Section~\ref{sec:evaluation} quantifies the overheads and performance gains of
%explored and previously unexplored metadata designs.
%Section~\ref{sec:related-work} places Cudele in the context of other related
%work.

