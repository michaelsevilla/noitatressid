%\section{Discussion}
%
%% Could we have implemented this on something other than CephFS?
%One question that comes up often in our work with Ceph is: ``Why do we always
%choose Ceph and can we implement this on another storage system?" Ceph is
%popular, robust, and open source -- getting it merged into mainline has a
%better chance of getting used than if we had built the system `clean-slate'
%from the ground up. Using Ceph, we can also leverage its robustness using an
%approach called ``programmability".
%
%% How awesome programmability is
%``Programmability" is a way of designing storage systems that encourages code
%re-use and composition.  A programmable storage system exposes internal
%subsystem as building blocks for higher level services. This `dirty-slate'
%approach limits redundant code and leverages the robustness of the underlying
%storage system. Cudele uses this approach and re-uses some of the building
%blocks from the Malacology programmable storage system~\cite{sevilla:eurosys17}
%to great success and requires only:
%
%\begin{itemize}
%
%  \item 354 lines of library code
%
%  \item 219 lines of non-destructive metadata server code, which is not used
%  unless it is turned on
%
%  \item 4 lines of destructive client/server code to check whether a namespace
%  is decoupled
%
%\end{itemize}

\section{Conclusion}

Relaxing consistency/durability semantics in the file system is a double-edged
sword. While it performs and scales better, it alienates applications that rely
on strong consistency and durability. Mounting other systems to the global
namespace is convenient but wastes resources and incurs data movement.  Cudele
lets users assign consistency/durability guarantees to subtrees in the global
namespace, which can be custom fit to the application. Using Cudele, we show
how applications can co-exist and perform well in a global namespace and lay
the groundwork for new systems that dynamically change consistency/durability
guarantees to avoid provisioning dedicated storage clusters and moving large
amounts of data.

% Implied Namespaces

% Intermediate Update Bursts

% See if this helps load balancing

% executing mechanisms in parallel
