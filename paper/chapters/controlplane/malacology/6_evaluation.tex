\section{Evaluation}
\label{sec:evaluation} 

%We wanted to implement CORFU and we had two choices: build it from the ground
%up or do it on top of Malacology. To show the feasability of building on
%Malacology our evaluation focuses on the 

Our evaluation demonstrates the feasibility of building new service
abstractions atop programmable storage, focusing on the performance of the
internal abstractions exposed by Malacology and used to construct the Mantle
and ZLog services. We also discuss latent capabilities we discovered in this
process that let us navigate different trade-offs within the services
themselves. First, we benchmark scenarios with high sequencer contention by
examining the interfaces used to map ZLog onto Malacology; specifically, we
describe the sequencer implementation and the propagation of object and data
interfaces interfaces.  Next, we benchmark scenarios in which the storage
system manages multiple logs by using Mantle to balance sequencers across a
cluster.

% should save this for future work, otherwise it undercuts the eval.
%Detailed system-wide performance measurements and analysis is forthcoming for
%both Mantle and ZLog, as we try to find the best metadata balancing policies
%and physical design implementation trade-offs.  

Since this work focuses on the programmability of Malacology, the goal of this
section is to show that the components and subsystems that support the
Malacology interfaces provide reasonable relative performance, as well as to
give examples of the flexibility that Malacology provides to programmers.  
This section uses a principled approach for evaluating tunables of the
interfaces and the trade-offs we discuss should be acknowledged when building
higher-level services.

%As we search for the best metadata balancing policies and physical design
%implementation trade-offs, we plan to rel

\subsection{Load Balancing ZLog Sequencers with Mantle}
\label{sec:zlog-balancing}

In practice, a storage system implementing CORFU will support a multiplicity of
independent totally-ordered logs for each application.  For this scenario
co-locating sequencers on the same physical node is not ideal but building a
load balancer that can migrate the shared resource (e.g., the resource
that mediates access to the tail of the log) is a time-consuming, non-trivial
task.  It requires building subsystems for migrating resources, monitoring the
workloads, collecting metrics that describe the utilization on the physical
nodes, partitioning resources, maintaining cache coherence, and managing
multiple sequencers. The following experiments demonstrate the feasibility of
using the mechanisms of the Malacology Load Balancing interface to inherit
these features and to alleviate load from overloaded servers.

\begin{figure}[t!]
\centering
\includegraphics{./chapters/controlplane/malacology/figures/mantle-balancer-behaviors.png}
\caption{
[\href{https://github.com/michaelsevilla/malacology-popper/blob/v2.1/experiments/mds-zlog-seq-migrate-redux-3client/results-mantle-runs/visualize.ipynb}{source}]
CephFS/Mantle load balancing have better throughput than co-locating all
sequencers on the same server.  Sections~\ref{sec:feature-balancing-modes}
and~\ref{sec:feature-migration-units} quantify this improvement;
Section~\ref{sec:feature-backoff} examines the migration at 0-60 seconds.
}\label{fig:mantle-balancer-behaviors}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics{./chapters/controlplane/malacology/figures/mantle-balancer-performance.png}
\includegraphics{./chapters/controlplane/malacology/figures/mantle-mode-performance.png}
\caption{
[\href{https://github.com/michaelsevilla/malacology-popper/blob/v2.1/experiments/mds-zlog-seq-migrate-redux-3client/results-mantle-runs/visualize.ipynb}{source},
\href{https://github.com/michaelsevilla/malacology-popper/blob/v2.1/experiments/mds-zlog-seq-migrate-redux-waves/results-paper/visualize.ipynb}{source}]
In (a) all CephFS balancing modes have the same performance; Mantle uses a
balancer designed for sequencers. In (b) the best combination of mode and
migration units can have up to a 2\(\times\)
improvement.}\label{fig:mantle-balancer-performance}
\end{figure}

The experiments are run on a cluster with 10 nodes to store objects, one node
to monitor the cluster, and 3 nodes that can accommodate sequencers.  Instead
of measuring contention at the clients like
Section~\ref{sec:sequencer-implementation}, these experiments measure
contention at the sequencers by forcing clients to make round-trips for every
request. We implement this using the Shared Resource interface that forces
round-trips.  Because the sequencer's only function is to  hand out positions
for the tail of the log, the workload is read-heavy.

First, we show how the ZLog service can orchestrate multiple sequencers using
the Malacology Load Balancing interface.
Figure~\ref{fig:mantle-balancer-behaviors} shows the throughput over time of
different load balancers as they migrate 3 sequencers (with 4 clients) around
the cluster; ``No Balancing" keeps all sequencers on one server, ``CephFS"
migrates sequencers using the hard-coded CephFS load balancers, and ``Mantle"
uses a custom load balancer we wrote specifically for sequencers.  The
increased throughput for the CephFS and Mantle curves between 0 and 60 seconds
are a result of migrating the sequencer(s) off overloaded servers.

In addition to showing that migrating sequencers improves performance,
Figure~\ref{fig:mantle-balancer-behaviors} also demonstrates features that we
will explore in the rest of this section.
Sections~\ref{sec:feature-balancing-modes}
and~\ref{sec:feature-migration-units} quantify the differences in performance
when the cluster stabilizes at time 100 seconds and
Section~\ref{sec:feature-backoff} examines the slope and start time of the
re-balancing phase between 0 and 60 seconds by comparing the aggressiveness of
the balancers.

\subsubsection{Feature: Balancing Modes}
\label{sec:feature-balancing-modes}

Next, we quantify the performance benefits shown in
Figure~\ref{fig:mantle-balancer-behaviors}.  To understand why load balancers
perform differently we need to explain the different balancing modes that the
load balancer service uses and how they stress the subsystems that receive and
forward client requests in different ways. In
Figure~\ref{fig:mantle-balancer-behaviors}, the CephFS curve shows the
performance of the balancing mode that CephFS falls into {\it most of the
time}.  CephFS currently has 3 modes for balancing load: CPU mode, workload
mode, and hybrid mode. All three have the same structure for making migration
decisions but vary based on the metric used to calculate load. For this
sequencer workload the 3 different modes all have the same performance, shown
in Figure~\ref{fig:mantle-balancer-performance} (a), because the load balancer
falls into the same mode a majority of the time.  The high variation in
performance for the CephFS CPU Mode bar reflects the uncertainty of using
something as dynamic and unpredictable as CPU utilization to make migration
decisions.  In addition to the suboptimal performance and unpredictability,
another problem is that all the CephFS balancers behave the same. This prevents
administrators from properly exploring the balancing state space.

Mantle gives the administrator more control over balancing policies; for the
Mantle bar in Figure~\ref{fig:mantle-balancer-performance} (a) we use the Load
Balancing interface to program logic for balancing read-heavy workloads,
resulting in better throughput and stability.  When we did this we also
identified two balancing modes relevant for making migration decisions for
sequencers. 

Using Mantle, the administrator can put the load balancer into ``proxy mode" or
``client mode". In proxy mode one server receives all requests and farms off
the requests to slave servers; the slave servers do the actual tail finding
operation. In client mode, clients interact directly with the server that has
their sequencer.  These modes are illustrated in Figure~\ref{fig:mantle-modes}.
``No Balancing" is when all sequencers are co-located on one physical server --
performance for that mode is shown by the ``No Balancing" curve in
Figure~\ref{fig:mantle-balancer-behaviors}. In ``Proxy Mode", clients continue
sending requests to server A even though some of the sequencers have been
migrated to another server. Server A redirects client requests for sequencer 2
to server B.  ``Proxy Mode (Half)" is shown in
Figure~\ref{fig:mantle-balancer-behaviors}; in this scenario, half of the
sequencers have migrated off the first server. Alternatively, ``Proxy Mode
(Full)", which is not pictured, is when all the sequencers migrate off the
first server.  ``Client Mode", shown on the far right of
Figure~\ref{fig:mantle-modes}, shows how clients for sequencer 2 contact server
B without a redirect from server A.

\begin{figure}[t!]
\centering
\includegraphics{./chapters/controlplane/malacology/figures/mantle-modes.png}
\caption{ In client mode clients sending requests to the server that houses
their sequencer. In proxy mode clients continue sending their requests to the
first server.  }\label{fig:mantle-modes}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics{./chapters/controlplane/malacology/figures/mantle-mode-behavior.png}
\caption{
[\href{https://github.com/michaelsevilla/malacology-popper/blob/v2.1/experiments/mds-zlog-seq-migrate-redux-waves/results-paper/visualize.ipynb}{source}]
The performance of proxy mode achieves the highest throughput but at the cost
of lower throughput for one of the sequencers. Client mode is more fair but
results in lower cluster throughput.  }\label{fig:mantle-mode-behavior}
\end{figure}

Figure~\ref{fig:mantle-mode-behavior} shows the throughput over time of the two
different modes for an environment with only 2 sequencers (again 4 clients
each) and 2 servers. The curves for both sequencers in
Figure~\ref{fig:mantle-mode-behavior}(a) start at less than 1000 ops/second and
at time 60 seconds Mantle migrates Sequencer 1 to the slave server.
Performance of Sequencer 2 decreases because it stayed on the proxy which now
processes requests for Sequencer 2, and forwards requests for Sequencer 1. The
performance of Sequencer 1 improves dramatically because distributing the
sequencers in this way separates (1) the handling of the client requests and
(2) finding the tail of the log and responding to clients.  Doing both steps is
too heavy weight for one server and sequencers on slave nodes can go faster if
work is split up; this phenomenon is not uncommon and has been observed in
chain replication~\cite{chain_rep}.

Cluster throughput improves at the cost of decreased throughput for Sequencer
2.  Figure~\ref{fig:mantle-mode-behavior}(b) is set to sequencer mode manually
(no balancing phase) and shows that the cluster throughput is worse than the
cluster throughput of proxy mode. That graph also shows that Sequencer 2 has
less throughput than Sequencer 1. In this case, the scatter-gather process used
for cache coherence in the metadata protocols causes strain on the server
housing Sequencer 2 resulting in this uneven performance. 

\subsubsection{Feature: Migration Units}
\label{sec:feature-migration-units}

Another factor that affects performance in this environment is how much load is
on each server; these experiments quantify that effect by programming the Load
Balancing interface to control the amount of load to migrate. We call this
metric a ``migration unit".  Expressing this heuristic is not easily achievable
with outward facing tunable parameters (i.e. system knobs) but with Mantle's
programmable interface, we can force the load balancer to change its migration
units. To force the balancer into the Proxy Mode (Half) scenario in
Figure~\ref{fig:mantle-modes}, which uses migration units equal to half the
load on the current server, we can use: \texttt{ targets[whoami+1] =
mds[whoami]["load"]/2 }.

This code snippet uses globally defined variables and tables from the Mantle
API~\cite{sevilla:sc15-mantle} to send half of the load on the current server
(whoami) to the next ranked server (whoami + 1); the \texttt{targets} array is
a globally defined table that the balancer uses to do the migrations.
Alternatively, to migrate all load a time step, we can remove the division by
2.

%\begin{figure}[t!]
%\centering
%\includegraphics{./chapters/controlplane/malacology/figures/mantle-mode-performance.png}
%\caption{The performance of the different modes using various migration units
%shows almost a 2\(\times\) improvement in the best case.
%}\label{fig:mantle-mode-performance}
%\end{figure}

Figure~\ref{fig:mantle-balancer-performance} (b) shows the performance of the
modes using different migration units. Recall that this setup only has 2
sequencers and 2 servers, so performance may be different at scale. Even so, it
is clear that client mode does not perform as well for read-heavy workloads. We
even see a throughput improvement when migrating all load off the first server,
leaving the first server to do administrative tasks (this is common in the
metadata cluster because the first server does a lot of the cache coherence
work) while the second server does all the processing. Proxy mode does the
best in both cases and shows large performance gains when completely decoupling
client request handling and operation processing in Proxy Mode (Full).  The
parameter that controls the migration units helps the administrator control the
sequencer co-location or distribution across the cluster. This trade-off was
explored extensively in the Mantle paper but the experiments we present here
are indicative of an even richer set of states to explore.

\subsubsection{Feature: Backoff}
\label{sec:feature-backoff}

Tuning the aggressiveness of the load balancer decision making is also a
trade-off that administrators can control and explore. The balancing phase from
0 to 60 seconds in Figure~\ref{fig:mantle-balancer-behaviors} shows different
degrees of aggressiveness in making migration decisions; CephFS makes a
decision 10 seconds into the run and throughput jumps to 2500 ops/second 
while Mantle takes more time to stabilize. This conservative behavior is
controlled by programming the balancer to (1) use different conditions for when
to migrate and (2) using a threshold for sustained overload. 

We control the conditions for when to migrate using \texttt{when()}, a callback
in the Mantle API.  For the Mantle curve in
Figure~\ref{fig:mantle-balancer-behaviors} we program \texttt{when()} to wait
for load on the receiving server to fall below a threshold. This makes the
balancer more conservative because it takes 60 seconds for cache coherence
messages to settle.  The Mantle curve in
Figure~\ref{fig:mantle-balancer-behaviors} also takes longer to reach peak
throughput because we want the policy to wait to see how migrations affect the
system before proceeding; the balancer does a migration right before 50
seconds, realizes that there is a third underloaded server, and does another
migration. 

The other way to change aggressiveness of the decision making is to program
into the balancer a threshold for sustained overload. This forces the balancer
to wait a certain number of iterations after a migration before proceeding. In
Mantle, the policy would use the save state function to do a countdown after a
migration.  Behavior graphs and performance numbers for this backoff feature is
omitted for space considerations, but our experiments confirm that the more
conservative the approach the less overall throughput.
 
\textbf{Malacology pulls} the load balancing service out of the storage system
to balance sequencers across a cluster. This latent capability also gives
future programmers the ability to explore the different load balancing
trade-offs including: load balancing modes to control forwarding vs. client
redirection, load migration units to control sequencer distribution vs.
co-location, and backoffs to control conservative vs. aggressive decision
making.

%\subsubsection{Experiment: Object Class
%Programmability}\label{experiment-object-class-programmability}
%
%Add quote from CORFU paper that talks about what they had to hack
%
%\subsubsection{Experiment: Multi-Client
%Burstiness}\label{experiment-multi-client-burstiness}
%
%\begin{figure}[htbp] \centering
%\includegraphics{./chapters/controlplane/malacology/figures/caps-delay-thruput.png} \caption{Forcing the client
%to drop their capabilities later (delay) improves throughput} \end{figure}
