\section{Implementing Mantle on Programmable Storage}
\label{sec:mantle-progstorage}

Mantle~\cite{sevilla:sc15-mantle} is a programmable load balancer that
separates the metadata balancing policies from their mechanisms. Administrators
inject code to change how the metadata cluster distributes metadata. Our
previous work showed how to use Mantle to implement a single node metadata
service, a distributed metadata service with hashing, and a distributed
metadata service with dynamic subtree partitioning. 

The original implementation was ``hard-coded" into Ceph and lacked robustness
(no versioning, durability, or policy distribution).  Re-implemented using
Malacology, Mantle now enjoys (1) the versioning provided by Ceph's monitor
daemons and (2) the durability and distribution provided by Ceph's reliable
object store.  Re-using the internal abstractions with Malacology resulted in a
2\(\times\) reduction in source code compared to the original implementation.

\subsubsection{Versioning Balancer Policies}

Ensuring that the version of the current load balancer is consistent across the
physical servers in the metadata cluster was not addressed in the original
implementation. The user had to set the version on each individual server and
it was trivial to make the versions inconsistent. Maintaining consistent
versions is important for cooperative balancing policies, where local decisions
are made assuming properties about other instances in the cluster.

With Malacology, Mantle stores the version of the current load balancer in the
Service Metadata interface. The version of the load balancer corresponds to an
object name in the balancing policy. Using the Service Metadata interface means
Mantle inherits the consistency of Ceph's internal monitor daemons. The user
changes the version of the load balancer using a new CLI command.

\subsubsection{Making Balancer Policies Durable}

The load balancer version described above corresponds to the name of an object
in RADOS that holds the actual Lua balancing code.  When metadata server nodes
start balancing load, they first check the latest version from the metadata
server map and compare it to the balancer they have loaded. If the version has
changed, they dereference the pointer to the balancer version by reading the
corresponding object in RADOS. This is in contrast to the original Mantle
implementation which stored load balancer code on the local file system -- a
technique which is unreliable and may result in silent corruption.

The balancer pulls the Lua code from RADOS synchronously; asynchronous reads
are not possible because of the architecture of the metadata server. The
synchronous behavior is not the default behavior for RADOS operations, so we
achieve this with a timeout: if the asynchronous read does not come back within
half the balancing tick interval the operation is canceled and a Connection
Timeout error is returned. By default, the balancing tick interval is 10
seconds, so Mantle will use a 5 second second timeout.

This design allows Mantle to immediately return an error if anything
RADOS-related goes wrong.  We use this implementation because we do not want to
do a blocking object storage daemon read from inside the global metadata server
lock. Doing so would bring down the metadata server cluster if any of the
object storage daemons are not responsive.

Storing the balancers in RADOS is simplified by the use of an interpreted
language for writing balancer code. If we used a language that needs to be
compiled, like the C++ object classes in the object storage daemon, we would
need to ensure binary compatibility, which is complicated by different
operating systems, distributions, and compilers.

\subsubsection{Logging, Debugging, and Warnings}

In the original implementation, Mantle would log all errors, warnings, and
debug messages to a log stored locally on each metadata server. To get the
simplest status messages or to debug problems, the user would have to log into
each metadata server individually, look at the logs, and reason about causality
and ordering.

With Malacology, Mantle re-uses the centralized logging features of the
monitoring service. Important errors, warnings, and info messages are collected
by the monitoring subsystem and appear in the monitor cluster log so instead of
users going to each node, they can watch messages appear at the monitor daemon.
Messages are logged sparingly, so as not to overload the monitor with frivolous
debugging but important events, like balancer version changes or failed
subsystems, show up in the centralized log.

