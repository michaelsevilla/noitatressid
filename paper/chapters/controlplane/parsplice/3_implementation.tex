%of timestamp (ts), value pairs representing accesses over time types of
\subsection{Integrating Mantle into ParSplice}

% env of metrics
Using Mantle cluster metrics, we expose cache size, CPU utilization, and memory
pressure of the worker node to the cache management policies. In
Section~\S\ref{sec:arch-specific} we only end up using the cache size although
the other metrics proved to be valuable debugging tools. Using Mantle time
series metrics, we expose accesses to the cache as a list of \texttt{timestamp,
key} pairs. In Section~\S\ref{sec:dom-specific}, we explore a key access
pattern detection algorithm that uses this metric.

% where policies are made
We link Mantle into all caches in the system and put the ``when" and ``how
much" callbacks alongside code that checks for memory pressure. It is executed
right before the worker processes incoming and outgoing put/get transactions to
the cache. We only do cache management once every second to avoid maintaining
the cache for every request. We expected to have to increase this polling
interval to accommodate more complex policies but even our most complicated
policy in Section~\S\ref{sec:dom-specific} had a negligible effect on
performance when executed every second (within the standard deviation for
multiple runs when compared against a policy that returns immediately). This
may be because the worker is not overloaded and the bottleneck is somewhere
else in the system.  As stated previously, we do not use the ``where" part of
Mantle because we focus on a single node, but this part of the API will be used
when we move the caches and storage nodes to a key-values store back-end that
uses key load balancing and repartitioning.

