% This has been done before
This idea is not new. As an example, consider the evolution of single node
operating systems. System speeds stopped doubling according to Moore's law
because the memory transfer rate (speed) and the number of onboard connections
(physical space) were limiting performance, a phenomenon known as the ``memory
wall"~\cite{wulf:sigarch1995-memory-wall}. As a result, systems took control
away from the user with new abstractions, such as multicore systems and virtual
memory. The system migrated the system resources and lied to the user about the
available resources. Although this comes at the comes cost of transparency
(i.e. requiring tools like PIR~\cite{olschanowsky:ICPPW2010-PIR},
Pin~\cite{luk:PLDI2005-pin}, and the Linux memory tool
suite~\cite{movall:atec2005-physical}), the community has come to the
realization that letting the system manage its own load and resources is better
than manually tuning knobs. 

% This is a problem in distributed systems
Today, we face a similar dilemma in distributed systems. Designers statically
configure large systems to ensure they can handle the intended load. This is a
three step process: (1) characterize the workloads, (2) select a scaling
architecture, and (3) repetitively tune system parameters until performance is
optimal. This process is especially difficult because of the scale of the data
and the complexity of the system. Characterizing workloads is usually only done
for the most popular systems, such as
Hadoop~\cite{chen:2012ccpe-distributed-db,chen:2012vldb-cross-industry,chen:2011mascots-suites,huang:icde2010-hibench}.
Selecting the best scaling architectures is difficult, even for seasoned
experts. For example, deciding whether to scale-out (by adding more nodes to
the system) or scale-up (by adding resources to a single node) is difficult
because many of the assumptions about yesterday's comparisons and
architectures~\cite{michael:2007pdps-scale-up-x-scale-out,talkington:2002journal-scaling,wisniewski:2007europar-commercial-scale-out}
are not true
today~\cite{appuswamy:socc2013-hadoop-vs-single-node,rowstron:hotcdp2012-hadoop-vs-single-node,sevilla:discs2013-framework,sevilla:lspp2014-supmr}.
Finally, resolving trade-offs by tuning system parameters is something of a
black art for large scale processing systems. Even for well-understood systems
such as Hadoop, understanding the trade-offs and the effects they have on
performance is
difficult~\cite{herodotou:cidr2011-starfish,wang:mascots2009-mrperf}.
Self-tuning systems have crept their way into frameworks such as MapReduce for
distributed processing~\cite{herodotou:cidr2011-starfish}, query analysis for
distributed databases~\cite{lefevre:danac2013-evolutionary-analytics}, and disk
arrays for distributed storage~\cite{anderson:fast2001-hippodrome}, but these
tools are domain specific and fail to balance load and resources across
multiple systems. 

%\footnote{I asked the authors of~\cite{appuswamy:socc2013-hadoop-vs-single-node} how they tune Hadoop and they said ``Oh, we hired an intern to do that".}

% Why we need migration
Resource migration is an important tool for dynamically balancing load and
resources across multiple systems. While many fields, such as cloud
computing~\cite{zhang:journal2010-cloud-challenges} and
databases~\cite{elmore:sigmod2013-pythia}, have shown the benefit of migrating
resources, they only look at migrating resource consumers ({\it i.e.} a virtual
machine, application, or process), instead of the resources themselves. Systems
research has already shown that moving compute resources, such as
cores~\cite{zhang:journal2010-cloud-challenges},
memory~\cite{chapman:atc2009-vnuma}, IO~\cite{raj:hpdc2007-io-virtualization},
and network~\cite{georgiadis:atn1996-network-qos}, has arrived. With such
revolutionary technology on the horizon, it is important that we start to
explore migrating the actual {\it resources}, instead of the resource
consumers.

% How load balancing is analogous to metadata management
Automatic load balancing and resource migration in a distributed system is
often times a multi-objective optimization problem. Many of the trade-offs,
such as maintaining global knowledge, managing cluster-wide trade-offs, and
accounting for the cost of migration, mirror general distributed systems
problems. When we finally have the ability to migrate different resources, how
do we know when and where to move them? Such migration will depend on the
utilization, configuration, and workload, but how will we weight these factors
to design robust, guaranteeable systems? To explore different heuristics for
resource migration and load balancing, we narrow the scope to look at an
analogous problem in distributed file systems: the metadata management problem.
We hypothesize that an effective metadata management strategy will also depend
on the utilization, configuration, and workload. 

%that the metadata management problem is sufficiently challenging~\ref{}, we show that the community is interested in metadata management~\ref{}, that we are are in a position to solve this problem~\ref{background},. 

%To reduce the the design phase, work that models such systems~\cite{wang:mascots2009-mrperf} has the intention of letting the developer experiment with different workloads before deploying a system that is statically configured for their job.
%Our contributions:
%\begin{enumerate}
%	\item we identify resource balancing challenges and present symptoms of poor load balancing 
%	\item we reflect on tradeoffs that affect load balancing, techniques for resolving these trade-offs, and opportunities for dynamically tuning these trade-offs
%	\item we motivate CephFS as a prototyping platform for migrating resources
%\end{enumerate}



% Solution:  systems balance their own loads
%% All the information is already in the system and these bottlenecks have been seen before. 
%It is understood that if a workload calls for more cores or memory, the solution is to provision the system with more cores and memory. But systems are so large and fast that statically provisioning resources and tuning parameters by hand is insufficient. Why do we a need a human to identify a bottleneck, diagnose the problem, and fix the problem? Why do we need a human to reconfigure and rebalance the system based on prior experiences? By letting the system balance its own load and resources, we can maximize scalability, performance, and efficiency. 



